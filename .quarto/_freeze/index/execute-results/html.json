{
  "hash": "a7a52316dbe6c48870004583a7a84011",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Backtesting overfitting\"\nsubtitle: \"\"\nauthor: \"Barry Quinn\"\nfooter: \"AI and Trading\"\nembed-resources: true\nlogo: \"img/qbslogo.png\"\nformat:\n  revealjs:\n    includes:\n    css: [\"mycssblend.css\"]\n    theme: default\n    transition: slide\n    slide-number: true\n    scrollable: true\n    width: 1600\n    height: 900\n    \nexecute:\n  echo: true\n---\n\n\n\n\n\n## Outline\n- Backtesting and selection bias under  multiple testing\n- Precision and recall in statistics\n- Neyman Pearson Type I and Type || errors under multiple hypothesis testing\n- False discovery\n- Most important simulation in quantitative finance\n\n\n## Experiment evidence using simulation\n\n::: {.saltinline}\n- So far we have used experimental evidence extensively.\n- More precisely we have used monte carlo simulations to allow us to reach conclusions regarding the mathematical properties of various estimators and algorithms under controlled conditions.\n- Good financial research requires the ability to control for the conditions of an experiment that can result in *realistic* causal inference statements.\n:::\n\n##  What is a backtest?\n\n- A backtest is a historical simulation of how an investment strategy would have performed in the past.\n- It is not a controlled experiment, because we cannot change the environmental variables to derive a new historical time series on which to perform an independent backtest. \n- As a result, backtests cannot help us derive the precise cause–effect mechanisms that make a strategy successful.\n- This identification issue is more than a techical inconvenience\n\n## Overfitting and statistical inflation\n\n- In the context of strategy development, all we have is a few (relatively short, serially correlated, multicollinear and possibly nonstationary) historical time series.\n- It is easy for a researcher to overfit a backtest, by conducting multiple historical simulations, and selecting the best performing strategy (Bailey et al. 2014). \n- When a researcher presents an overfit backtest as the outcome of a single trial, the simulated performance is inflated. \n- [This form of statistical inflation is called selection bias under multiple testing (SBuMT).]{.content-box-yellow}\n- SBuMT leads to false discoveries: strategies that are replicable in backtests, but fail when implemented.\n\n## Backtest hyperfitting {.small}\n\n- SBuMT is compounded as a consequence of sequential SBuMT at two levels:\n\n::: {.context-box-red}\n1. Each researcher runs millions of simulations, and presents the best (overfit) ones to her boss\n2. The company further selects a few backtests among the (already overfit) backtests submitted by the researchers. \n- We may call this backtest hyperfitting, to differentiate it from backtest overfitting (which occurs at the researcher level).\n- It may take many decades to collect the future (out-of-sample) information needed to debunk a false discovery that resulted from SBuMT.\n- In this lecture we study how researchers can estimate the effect that SBuMT has on their findings.\n:::\n\n\n##  Performance statistics {.small}\n\n\n| Performance Statistic | Description |\n|----------------------|-------------|\n| PnL | The total amount of dollars (or the equivalent in the currency of denomination) generated over the entirety of the backtest, including liquidation costs from the terminal position. |\n| PnL from long positions | The portion of the PnL dollars that was generated exclusively by long positions. |\n| Annualized rate of return | The time-weighted average annual rate of total return, including dividends, coupons, costs, etc. |\n| Hit ratio | The fraction of bets that resulted in a positive PnL. |\n| Average return from hits | The average return from bets that generated a profit. |\n| Average return from misses | The average return from bets that generated a loss. |\n\n##  Risk statistics {.small}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- Intuitively, a drawdown (DD)is the maximum loss suffered by an investment between two consecutive high-watermarks (HWMs).\n- The time under water (TuW) is the time elapsed between an HWM and the moment the PnL exceeds the previous maximum PnL.\n- In workshop 4 we used `PortfolioAnalytics` and chart the performance of our competing strategies.\n:::\n\n::: {.column width=\"40%\"}\n![](img/chart-1.png){.center}\n- You can see the drawdown statistics in the bottom graph\n:::\n\n::::\n\n## Implementation shortfall statistics {.small}\n\n\n#### Broker fees per turnover\n\n- Broker fees per turnover: These are the fees paid to the broker for turning the portfolio over, including exchange fees.\n\n#### Average slippage per turnover\n\n- Average slippage per turnover: These are execution costs, excluding broker fees, involved in one portfolio turnover.\n\n#### Dollar performance per turnover\n\n- Dollar performance per turnover: This is the ratio between dollar performance (including brokerage fees and slippage costs) and total portfolio turnovers.\n\n#### Return on execution costs\n\n- Return on execution costs: This is the ratio between dollar performance (including brokerage fees and slippage costs) and total execution costs.\n\n\n##   Efficiency statistics {.small}\n\n::: {.blockquote .center}\nEfficiency statistics provide a relative analysis of the performance of a backtest.\n:::\n\n\n#### Annualized Sharpe ratio\n\n- Annualized Sharpe ratio: This is the SR value, annualized by a multiplying by $\\sqrt{a}$ (a=average number of returns observations per year).\n\n### Information ratio\n\n- Information ratio: This is the SR equivalent of a portfolio that measures its performance relative to a benchmark.\n\n### Probabilistic Sharpe ratio\n\n- Probabilistic Sharpe ratio: PSR corrects SR for inflationary effects caused by non-Normal returns or track record length.\n\n### Deflated Sharpe ratio\n\n- Deflated Sharpe ratio: DSR corrects SR for inflationary effects caused by non-Normal returns, track record length, and selection bias under multiple testing.\n\n## Precision and Recall in Statistics {.small}\n\n- To understand how false discoveries affect performance in algorithmic trading and investment, we must first introduce two concepts. \n- In machine learning statistics, precision and recall are measures of task specific accuracy, especially in classification problems.\n- In terms of investment strategies:\n\n::: {.blockquote}\nprecision is the estimated probability that a randomly selected investment strategy from the pool of all positive backtests is a true strategy.\n:::\n\n::: {.blockquote}\nrecall (or true positive rate) is the estimated probability that a strategy randomly selected from the pool of true strategy has a positive backtest\n:::\n\n##  The Neyman-Pearson Framework {.small}\n\nUnder the standard Neyman-Pearson [1933] hypothesis testing framework:\n\n::: {.blockquote .small}\n- We state a null hypothesis H<sub>0</sub>, and an alternative hypothesis H<sub>1</sub>\n- We derive the distribution of a test statistic under H<sub>0</sub> and under H<sub>1</sub>\n- We reject H<sub>0</sub> with confidence $1-\\alpha$ in favour of H<sub>1</sub> when we observe an event that, should H<sub>0</sub> be true, should only occur with probability $\\alpha$\n:::\n\n- [This framework is the statistical analogue to a **proof by contradiction** argument]{.heatline}\n- There are 4 probabilities associated with a predicted positive $x >\\tau_{\\alpha}$ \n\n::: {.blockquote .small}\n- $Pr(x >\\tau_{\\alpha}|H_0)=\\alpha$ the type I error probability, or significance or false positive rate\n- $Pr(x >\\tau_{\\alpha}|H_1)=1-\\beta$ is the power of the test, recall or true positive rate, $Pr(x \\leq\\tau_{\\alpha}|H_1)=\\beta$ is the type II error probability or false negative rate\n- $Pr(H_0|x>\\tau_{\\alpha})$ the false discovery rate (FDR)\n- $Pr(H_1|x>\\tau_{\\alpha})$ the test's precision\n:::\n\n- Note again that p-value $\\alpha$ does not give the probability that the null hypothesis is true.\n\n##  A mathematical argument (Lopez de Prado 2020) {.small}\n\n- Let's say you have $s$ investment strategies to analyze as a quant researcher. \n- Inevitably, some of these strategies are false discoveries, in the sense that their expected return is not positive. \n- Mathematically, we can denote:\n\n$$s=s_T+s_F \\\\ \\text{vvwhere } \\\\ s_T=\\text{number of true strategies} \\\\ s_F=\\text{number of false strategies}$$\n\n- Let $\\theta$ be the odds ratio of true strategies against false strategies, $\\theta=s_T/s_F$. \n\n## A mathematical argument (Lopez de Prado 2020) {.small}\n\n- In finance, where the signal-to-noise ratio is low, false strategies abound, hence $θ$ is expected to be low. The number of true investment strategies is:\n\n$$S_T=s\\times \\frac{s_T}{s_T+s_F}$$\n\n- Likewise, the number of false investment strategies is: \n\n$$S_F=S-S_T=s \\left( 1-\\frac{\\theta}{(1+\\theta)}\\right)=s\\frac{1}{(1+\\theta)} $$\n\n- Given a false positive rate $\\alpha$ (type I error), we will obtain a number of false positives, $FP=\\alpha\\times S_F$, and a number of true negatives, $TN=(1-\\alpha)s_F$. \n\n##  A mathematical argument (Lopez de Prado 2020) {.small}\n\n- Denote $\\beta$ the false negative rate (type II error) associated with that $\\alpha$. \n- We will obtain a number of false negatives, $FN=\\beta \\times s_F$, and a number of true positives, $TP=(1-\\beta)s_T$.\n- Thus:\n\n::: {.blockquote}\n$$\\text{precision}=\\frac{TP}{(TP+FP)} = \\frac{(1-\\beta)s_T}{(1+\\beta)s_T+\\alpha s_F} \\\\ =\\frac{(1-\\beta)s\\frac{\\theta}{(1+\\theta)}}{(1-\\beta)s\\frac{\\theta}{(1+\\theta)}+\\alpha s\\frac{\\theta}{(1+\\theta)}}=\\frac{(1-\\beta)\\theta}{(1-\\beta)\\theta+\\alpha}$$\n:::\n\n::: {.blockquote}\n$$\\text{recall}=\\frac{TP}{(TP+FN)}=\\frac{(1-\\beta)s_T}{(1-\\beta)s_T+\\beta s_T}=1-\\beta$$\n:::\n\n## A mathematical argument (Lopez de Prado 2020) {.small}\n\n- What the mathematical logic tells us is before running backtests on a strategy, researchers should gather evidence that a strategy may indeed exist. \n- The reason is, the precision of the test is a function of the odds ratio $\\theta$. \n- If the odds ratio is low, the precision will be low, even if we get a positive with high confidence (low p-value). \n\n::: {.blockquote}\nThis is evidence to the pitfall that p-values report a rather uninformative probability. It is possible for a statistical test to have high confidence (low p-value) and low precision.\n:::\n\nIn particular, a strategy is more likely false than true if $(1-\\beta)\\theta < \\alpha$ such that precision is less than 50%.\n\n- Finally, there is an important relationship between the false discovery rate (FDR) and precision.\n- Specifically, \n\n$$FDR=\\frac{FP}{(FP+TP)}=\\frac{\\alpha}{(1-\\beta)\\theta+\\alpha}=1-precision$$\n\n##  A FDR function {.small}\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n- The following is a simple function which calculates precision, recall and the false discovery rate. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfdr_anal <- function(ground_truth, alpha = 0.05, beta, trails) {\n  theta = ground_truth / (1 - ground_truth)\n  recall = 1 - beta          \n  b1 = recall * theta\n  precision = b1 / (b1 + alpha)\n  tibble(Recall = recall, Precision = precision, FDR = 1 - precision)\n}\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n- Suppose before running backtests on a strategy, the researcher knows the *truth* that there is a 1% chance that the strategy is profitable.\n- If she sticks with the standard convention of 5% significance level and a 20% chance of a false negative, and runs 1000 trails, what is the rate of false discoveries?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfdr_anal(0.01, beta = 0.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  Recall Precision   FDR\n   <dbl>     <dbl> <dbl>\n1    0.8     0.139 0.861\n```\n\n\n:::\n:::\n\n\n- For this reason alone, we should expect that most discoveries in financial econometrics are likely false.\n:::\n\n:::\n\n## Familywise Error Rate (FWER) {.small}\n\n- When Neyman and Pearson [1933] proposed this framework, they did not consider the possibility of conducting multiple tests and select the best outcome.\n- When a test is repeated multiple times, the combined $\\alpha$ increases.\n- Consider that we repeat for a second time a test with false positive probability $\\alpha$.\n- At each trial, the probability of not making a Type I error is $1-\\alpha$\n- If the two trials are independent, the probability of not making a Type I error on the first and second tests is $(1-\\alpha)^2$\n- The probability of making *at least one* Type I error is the complementary, $1-(1-\\alpha)^2$\n- After a *family* of K independent tests, we reject H<sub>0</sub> with confidence $(1-\\alpha)^K$\n\n- FWER the probability that at least one of the positives is false, $\\alpha_K=1-(1-\\alpha)^K$\n\n- The Sidak Correction: for a given K and $\\alpha_K$ then $\\alpha=1-(1-\\alpha_K)^{1/K}$\n\n\n## FWER vs FDR {.small}\n\n- Thus far we have defined 2 Type 1 errors for multiple testing:\n1. Familywise Error Rate (FWER): The probability that at least one false positive takes place.\n2. False Discovery Rate (FDR): Expected value of the ratio of false positives to predicted positives.\n- In most scientific and industrial applications, FWER is considered overly punitive.\n  - For example, it would be impractical to design a car model where we control for the probability that a single unit will be defective.\n\n## FWER vs FDR {.small}\n  \n- However, in the context of finance, the FDR is preferrred as an investor does not typically allocate funds to all strategies with predicted positives within a family of trials, where a proportion of them are likely to be false.\n\n- Instead, investors are only introduced to the single best strategy out of a family of thousands or even millions of alternatives\n\n- Investors have no ability to invest in the discarded predicted positives.\n\n- Following the car analogue, in finance there is actually a single car unit produced per model, which everyone will use. If the only produced unit is defective, everyone will crash.]\n\n\n## What does this all mean for quantitative finance\n\n- Selection bias under multiple backtesting makes it impossible to assess the probability that a strategy is false.\n\n- Lopez de Prado (2018) argues that this explains why most quantitative investment firms fail as they are likely investing in false positives\n\n- This is because most financial analysts typically assess performance on the Sharpe ratio, not precision and recall.\n\n- Lopez de Prado (2020) develops a framework to assess the probability that a strategy is false, using the Sharpe ratio estimate and metadata from the discovery process as inputs\n\n\n## The golden age of the Sharpe Ratio (1966-2012) {.small}\n\n- In 1966, William Sharpe proposed a ratio metric that would come to dominate investment strategy appraisal \n- Consider an investment strategy with excess returns (or risk premia) $r_t, t=1,...,T$ which follows an IID Normal distribution\n$$ r_t \\sim N(\\mu,\\sigma)$$\n- Non-annualised SR of such a strategy is defined as \n$$SR=\\frac{\\mu}{\\sigma}$$\n- as the parameters $\\mu \\text{ and } \\sigma$ are unknown, they must be estimated such that SR is estimated as:\n\n$$\\hat{SR}=\\frac{E(r_t)}{\\sqrt{V_{r_t}}}$$\n\n## 2002 Andrew Lo and Elmar Mertens {.small}\n\n- [Andrew Lo](https://mitsloan.mit.edu/faculty/directory/andrew-w-lo) show that under the assumption that $r_t \\overset{IID}{\\sim} N(\\mu,\\sigma)$ the asymptotic  distribution of $\\hat{SR}$ is\n\n$$(\\hat{SR}-SR) \\overset{a}{\\to} N \\left[0,\\frac{1+0.5SR^2}{T}\\right]$$\n\n- Subsequent evidence showed hedge fund returns exhibit substantial negative skewness, and positive excess kurtosis.\n- the implication being that assumed IID normal returns will grossly underestimate the false positive probability\n\n- [Elmar Mertons](http://www.elmarmertens.com/) then derived an asymptotic distribution for $\\hat{SR}$ that include a variance terms which incorporated skewness and kurtosis.\n\n\n## 2012 David Bailey and Marco lopez de Prado {.small}\n\n- In the Journal of Risk, [David Bailey](https://www.davidhbailey.com/) and [Marco Lopez de Prado](https://www.quantresearch.org/) utilises previous results to derive the [Probabilistic Sharpe Ratio](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1821643)\n\n::: {.blockquote}\n- PSR estimates the probability that the observed $\\hat{SR}$ exceeds SR* as:\n\n$$\\hat{PSR}(SR*)=Z\\left(\\frac{(\\hat{SR}-SR*)\\sqrt{T-1}}{\\sqrt{1-\\hat{\\gamma_3}\\hat{SR}+\\frac{\\hat{\\gamma_4}-1}{4}\\hat{SR}^2}}\\right)$$\n\n- where Z[.] is the cumulative density function of the standard Normal distribution, T is number of observed returns, and $\\hat{SR}$ is the non-annualised estimate of SR, computed on the same frequency as the T observations. \n\n\n## Inference on the Probabilistic Sharpe Ratio\n\n- For a given SR*, the probabilistic sharpe ratio increases with greater mean returns, lower variance of returns, longer track record (T), positively skewed returns, and thinner tails\n\n## The False Strategy Theorem\n\n- Bailey et al. (2014) formalised a theorem ,*False Strategy Theorem* , that expressed the SBuMT as a function on the number of trails and the variance of the Sharpe ratios.\n\n::: {.blockquote}\n\n- In practice a researcher may carry out a large number of historical simulations (trails) and report only the best outcome (maximum Sharpe ratio)\n- Maximum Sharpe ratio is not randomly distributed which gives rise to *SBuMT*, so when more than one trail takes place the maximum Sharpe ration is greater than the expected value of the Sharpe ration from a random trail.\n- The theorem shows that given a investment strategy with an expected Sharpe ratio of zero and non-zero variance, the expected value of the maximum Sharpe ratio is strictly positive and a function of the number of trails\n:::\n\n## The False Strategy Theorem\n\n- Given a sample of IID-Gaussian Sharpe ratios $\\widehat{SR_k},k=1,..,K \\text{  with } \\widehat{SR_k} \\sim N(0,V(\\widehat{SR_k})$\n\n\n$$E(\\underset{k}max(\\widehat{SR_k}))V(\\widehat{SR_k}^{-0.5} \\approx (1-\\gamma)Z^{-1} \\left[1-\\frac{1}{K}\\right]+\\gamma Z^{-1}\\left[1-\\frac{1}{Ke}\\right] $$\n- where $Z^{-1}$ is the inverse of the standard Gaussian CDF, e is Euler's number, and $\\gamma $is the Euler-Mascheroni constant.\n\n- **Corollary:** Unless $\\underset{k}max(\\widehat{SR_k}) >> E(\\underset{k}max(\\widehat{SR_k}))$  the discovered strategy is likely to be a false positive.\n\n- But $E(\\underset{k}max(\\widehat{SR_k}))$ is usually unknown, ergo SR is dead.\n\n\n## The *False Strategy* theorem\n- .Lopez de Prado (2020) `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z\"/></svg>`{=html} code\n- The theorem can be used to express the magnitude of the SBuMT as the difference between the expected maximum Sharpe ratio and the expected Sharpe ratio of a *false* strategy from a random trail\n\n\n## The *False Strategy* theorem  `R`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngetExpectedMaxSR<-function(nTrails,meanSR,stdSR){\n  # Expected Max SR controlling for SBuMT\n  emc=0.577215664901532860606512090082402431042159336\n  sr0=(1-emc)*qnorm(p=1-1./nTrails)+emc*qnorm(1-(nTrails*exp(1))^(-1))\n  sr0=meanSR+stdSR*sr0\n  return(sr0)\n}\n```\n:::\n\n\n## Distribution of Maximum SR\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngetDistMaxSR<-function(nSims,nTrails,meanSR,stdSR){\n  out=tibble(\"Max{SR}\"=NA,\"nTrails\"=NA)\n  for (nTrails_ in nTrails) {\n    #1) Simulated Sharpe Ratios\n    set.seed(nTrails_)\n    sr<-array(rnorm(nSims*nTrails_),dim = c(nSims,nTrails_))\n    sr<-apply(sr,1,scale) # demean and scale\n    sr= meanSR+sr*stdSR\n    #2) Store output\n    out<-out %>% bind_rows(\n      tibble(\"Max{SR}\"=apply(sr,2,max),\"nTrails\"=nTrails_))\n  }\n  return(out)\n}\n```\n:::\n\n\n## Run the experiment\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(pracma)\n# Create a sequential on the log-linear scale\nnTrails<-as.integer(logspace(1,4,100)) %>% unique()\nplot(nTrails)\nsr0=array(dim = length(nTrails))\nfor (i in seq_along(nTrails)) {\n  sr0[i]<-getExpectedMaxSR(nTrails[i],meanSR = 0, stdSR = 1)\n}\nsr1=getDistMaxSR(nSims = 1000,nTrails = nTrails,meanSR = 0,stdSR = 1)\n```\n:::\n\n\n## Most important plot in Quantitative finance {.small}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n<img src=\"img/maxSR.png\">\n\n\n## Inference from plot {.small}\n- The experiment compares the empirical (Monte Carlo) estimate of Maximum Sharpe ratio under the null of a false strategy to that implied by the FS theorem\n- The plot shows the output of the experiment for 1 to 10,000 trails.\n- The code sets $V[\\hat{SR_k}]=1$ and simulates the maximum Sharpe ratio 500 times, to derive a distribution of maximum Sharpe ratios for any k (number of trails).\n- the y axis shows the distribution of the $max_k(\\hat{SR_k})$ and the Expect  \n- this results is profound, after only 100 independent backtests the expected maximum Sharpe ratio is 3.2, even when the true Sharpe ratio is zero.\n- The reason is **Backtest overfitting**: when selection bias (picking the best results) takes place under multiple testing (running many alternative configurations) that backtests are likely to be false discoveries.\n\n\n## A Solution {.small}\n\n- [The Deflated Sharpe Ratio](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2460551) computes the probability that the Sharpe Ratio (SR) is statistically significant.\n\n$$\\widehat{DSR} \\equiv \\widehat{PSR}(\\widehat{SR_0})=Z \\left[\\frac{(\\hat{SR}-E[max_k(\\widehat{SR_k})])\\sqrt{T-1}}{\\sqrt{1-\\hat{\\gamma_3}\\widehat{SR}+\\frac{\\hat{\\gamma_4}-1}{4}\\widehat{SR}^2}}\\right]$$ \n- $\\widehat{DSR}$ can be interpreted as the probability of observing a Sharpe ratio greater or equal to $\\widehat{SR}$ subject to the null hypothesis that the true Sharpe ratio is zero, while adjusting for skewness $\\gamma_3$, kurtosis $\\gamma_4$, sample length and multiple testings.\n\n- Calculate DSR requires the estimation $E[max_k(\\widehat{SR_k})])$ which requires estimating $K$ and \n$V(\\hat{SR})$ which is where FML can help.\n\n- Specifically, we are employ optimal number of clustering to estimate K the effective number of trails and then calculate the variances.  \n\n\n## Implications for Academics\n\n::: {.columns}\n::: {.column width=\"50%\"}\n- Most studies in empirical finance are false (Harvey et al., 2016)\n- Selection bias may invalidate the entire body of work performed for the past 100 years\n- Finance cannot survive as a discipline unless we solve this problem\n- Investors and regulators have no reason to trust the value added by researchers and asset managers unless we learn to prevent false discoveries\n:::\n\n::: {.column width=\"50%\"}\n- Applying the False Strategy theorem to prevent false positives in finance\n- Requires estimating two meta-research variables to discount for \"lucky findings\"\n- Academic journals should cease accepting papers that do not control for selection bias under multiple testing\n- Papers must report the probability that the claimed financial discovery is a false positive\n:::\n:::\n\n## Implications for Regulators\n\n- Before the FDA, adulteration and mislabeling of food and drugs caused frequent episodes of mass poisoning, birth defects, and death\n- Financial firms engaging in backtest overfitting defraud investors for tens of billions of dollars annually\n- The SEC could demand quantitative firms certify the probability that promoted investments are bogus\n- Quantitative firms should be required to store all trials involved in a discovery for post-mortem analysis\n\n## Implications for Investors\n\n- Many financial firms promote pseudo-scientific products as scientific\n- Investment products based on award-winning journal articles are not necessarily scientific\n- If the original author has not become rich with the discovery, investors' chances are slim\n- Investors should demand firms report the results of all trials, not only the best-looking ones\n- Investors should consult databases of investment forecasts and assess the credibility of gurus and financial firms based on all outcomes from past predictions\n\n## References\n\n- Gu, Shihao, Bryan Kelly, and Dacheng Xiu. 2020. \"Empirical Asset Pricing via Machine Learning.\" The Review of Financial Studies.\n- Harvey, Campbell R., Presidential Address: The Scientific Outlook in Financial Economics. 2017.\n- American Statistical Association. 2016. \"Ethical guidelines for statistical practice.\"\n- López de Prado, M. and M. Lewis. 2018. \"Detection of False Investment Strategies Using Unsupervised Learning Methods.\"\n- Bailey, D., J. Borwein, M. López de Prado, and J. Zhu. 2014. \"Pseudo-mathematics and financial charlatanism: The effects of backtest overfitting on out-of-sample performance.\"\n- Bailey, D., J. Borwein, M. López de Prado, and J. Zhu. 2017. \"The Probability of Backtest Overfitting.\"\n- Bailey, D. and M. López de Prado. 2012. \"The Sharpe ratio efficient frontier.\"\n- Bailey, D. and M. López de Prado. 2014. \"The deflated Sharpe ratio: Correcting for selection bias, backtest overfitting and non-normality.\"\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}