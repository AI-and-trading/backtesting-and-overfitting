{
  "hash": "a0249665da96314fed265601d954366e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Backtesting overfitting\"\nsubtitle: \"\"\nauthor: \"Barry Quinn\"\nfooter: \"AI and Trading\"\nembed-resources: true\nlogo: \"img/qbslogo.png\"\nformat:\n  revealjs:\n    includes:\n    css: [\"mycssblend.css\"]\n    theme: default\n    transition: slide\n    slide-number: true\n    width: 1600\n    height: 900\n    \nexecute:\n  echo: true\n---\n\n\n\n\n\n## Outline\n- Backtesting and selection bias under  multiple testing\n- Precision and recall in statistics\n- Neyman Pearson Type I and Type || errors under multiple hypothesis testing\n- False discovery\n- Most important simulation in quantitative finance\n\n\n## Experiment evidence using simulation\n\n::: {.saltinline}\n- So far we have used experimental evidence extensively.\n- More precisely we have used monte carlo simulations to allow us to reach conclusions regarding the mathematical properties of various estimators and algorithms under controlled conditions.\n- Good financial research requires the ability to control for the conditions of an experiment that can result in *realistic* causal inference statements.\n:::\n\n##  What is a backtest?\n\n- A backtest is a historical simulation of how an investment strategy would have performed in the past.\n- It is not a controlled experiment, because we cannot change the environmental variables to derive a new historical time series on which to perform an independent backtest. \n- As a result, backtests cannot help us derive the precise cause–effect mechanisms that make a strategy successful.\n- This identification issue is more than a techical inconvenience\n\n## Overfitting and statistical inflation\n\n- In the context of strategy development, all we have is a few (relatively short, serially correlated, multicollinear and possibly nonstationary) historical time series.\n- It is easy for a researcher to overfit a backtest, by conducting multiple historical simulations, and selecting the best performing strategy (Bailey et al. 2014). \n- When a researcher presents an overfit backtest as the outcome of a single trial, the simulated performance is inflated. \n- [This form of statistical inflation is called selection bias under multiple testing (SBuMT).]{.content-box-yellow}\n- SBuMT leads to false discoveries: strategies that are replicable in backtests, but fail when implemented.\n\n## Backtest hyperfitting {.small}\n\n- SBuMT is compounded as a consequence of sequential SBuMT at two levels:\n\n::: {.context-box-red}\n1. Each researcher runs millions of simulations, and presents the best (overfit) ones to her boss\n2. The company further selects a few backtests among the (already overfit) backtests submitted by the researchers. \n- We may call this backtest hyperfitting, to differentiate it from backtest overfitting (which occurs at the researcher level).\n- It may take many decades to collect the future (out-of-sample) information needed to debunk a false discovery that resulted from SBuMT.\n- In this lecture we study how researchers can estimate the effect that SBuMT has on their findings.\n:::\n\n\n##  Performance statistics\n\nSure! Here's the updated table without the `[]`, `{}`, and their content:\n\n| Performance Statistic | Description |\n|----------------------|-------------|\n| PnL | The total amount of dollars (or the equivalent in the currency of denomination) generated over the entirety of the backtest, including liquidation costs from the terminal position. |\n| PnL from long positions | The portion of the PnL dollars that was generated exclusively by long positions. |\n| Annualized rate of return | The time-weighted average annual rate of total return, including dividends, coupons, costs, etc. |\n| Hit ratio | The fraction of bets that resulted in a positive PnL. |\n| Average return from hits | The average return from bets that generated a profit. |\n| Average return from misses | The average return from bets that generated a loss. |\n\n##  Risk statistics\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- Intuitively, a drawdown (DD)is the maximum loss suffered by an investment between two consecutive high-watermarks (HWMs).\n- The time under water (TuW) is the time elapsed between an HWM and the moment the PnL exceeds the previous maximum PnL.\n- In workshop 4 we used `PortfolioAnalytics` and chart the performance of our competing strategies.\n:::\n\n::: {.column width=\"40%\"}\n![](img/chart-1.png){.center}\n- You can see the drawdown statistics in the bottom graph\n:::\n\n::::\n\n## Implementation shortfall statistics\n\n\n#### Broker fees per turnover\n\n- Broker fees per turnover: These are the fees paid to the broker for turning the portfolio over, including exchange fees.\n\n#### Average slippage per turnover\n\n- Average slippage per turnover: These are execution costs, excluding broker fees, involved in one portfolio turnover.\n\n#### Dollar performance per turnover\n\n- Dollar performance per turnover: This is the ratio between dollar performance (including brokerage fees and slippage costs) and total portfolio turnovers.\n\n#### Return on execution costs\n\n- Return on execution costs: This is the ratio between dollar performance (including brokerage fees and slippage costs) and total execution costs.\n\n\n##   Efficiency statistics\n\n::: {.blockquote .center}\nEfficiency statistics provide a relative analysis of the performance of a backtest.\n:::\n\n\n#### Annualized Sharpe ratio\n\n- Annualized Sharpe ratio: This is the SR value, annualized by a multiplying by $\\sqrt{a}$ (a=average number of returns observations per year).\n\n### Information ratio\n\n- Information ratio: This is the SR equivalent of a portfolio that measures its performance relative to a benchmark.\n\n### Probabilistic Sharpe ratio\n\n- Probabilistic Sharpe ratio: PSR corrects SR for inflationary effects caused by non-Normal returns or track record length.\n\n### Deflated Sharpe ratio\n\n- Deflated Sharpe ratio: DSR corrects SR for inflationary effects caused by non-Normal returns, track record length, and selection bias under multiple testing.\n\n## Precision and Recall in Statistics\n\n- To understand how false discoveries affect performance in algorithmic trading and investment, we must first introduce two concepts. \n- In machine learning statistics, precision and recall are measures of task specific accuracy, especially in classification problems.\n- In terms of investment strategies:\n\n::: {.blockquote}\nprecision is the estimated probability that a randomly selected investment strategy from the pool of all positive backtests is a true strategy.\n:::\n\n::: {.blockquote}\nrecall (or true positive rate) is the estimated probability that a strategy randomly selected from the pool of true strategy has a positive backtest\n:::\n\n##  The Neyman-Pearson Framework\n\nUnder the standard Neyman-Pearson [1933] hypothesis testing framework:\n\n::: {.blockquote .small}\n- We state a null hypothesis H<sub>0</sub>, and an alternative hypothesis H<sub>1</sub>\n- We derive the distribution of a test statistic under H<sub>0</sub> and under H<sub>1</sub>\n- We reject H<sub>0</sub> with confidence $1-\\alpha$ in favour of H<sub>1</sub> when we observe an event that, should H<sub>0</sub> be true, should only occur with probability $\\alpha$\n:::\n\n- [This framework is the statistical analogue to a **proof by contradiction** argument]{.heatline}\n- There are 4 probabilities associated with a predicted positive $x >\\tau_{\\alpha}$ \n\n::: {.blockquote .small}\n- $Pr(x >\\tau_{\\alpha}|H_0)=\\alpha$ the type I error probability, or significance or false positive rate\n- $Pr(x >\\tau_{\\alpha}|H_1)=1-\\beta$ is the power of the test, recall or true positive rate, $Pr(x \\leq\\tau_{\\alpha}|H_1)=\\beta$ is the type II error probability or false negative rate\n- $Pr(H_0|x>\\tau_{\\alpha})$ the false discovery rate (FDR)\n- $Pr(H_1|x>\\tau_{\\alpha})$ the test's precision\n:::\n\n- Note again that p-value $\\alpha$ does not give the probability that the null hypothesis is true.\n\n##  A mathematical argument (Lopez de Prado 2020)\n\n- Let's say you have $s$ investment strategies to analyze as a quant researcher. \n- Inevitably, some of these strategies are false discoveries, in the sense that their expected return is not positive. \n- Mathematically, we can denote:\n\n$$s=s_T+s_F \\\\ \\text{where } \\\\ s_T=\\text{number of true strategies} \\\\ s_F=\\text{number of false strategies}$$\n\n- Let $\\theta$ be the odds ratio of true strategies against false strategies, $\\theta=s_T/s_F$. \n\n## A mathematical argument (Lopez de Prado 2020)\n\n- In finance, where the signal-to-noise ratio is low, false strategies abound, hence $θ$ is expected to be low. The number of true investment strategies is:\n\n$$S_T=s\\times \\frac{s_T}{s_T+s_F}$$\n\n- Likewise, the number of false investment strategies is: \n\n$$S_F=S-S_T=s \\left( 1-\\frac{\\theta}{(1+\\theta)}\\right)=s\\frac{1}{(1+\\theta)} $$\n\n- Given a false positive rate $\\alpha$ (type I error), we will obtain a number of false positives, $FP=\\alpha\\times S_F$, and a number of true negatives, $TN=(1-\\alpha)s_F$. \n\n##  A mathematical argument (Lopez de Prado 2020)\n\n- Denote $\\beta$ the false negative rate (type II error) associated with that $\\alpha$. \n- We will obtain a number of false negatives, $FN=\\beta \\times s_F$, and a number of true positives, $TP=(1-\\beta)s_T$.\n- Thus:\n\n::: {.blockquote}\n$$\\text{precision}=\\frac{TP}{(TP+FP)} = \\frac{(1-\\beta)s_T}{(1+\\beta)s_T+\\alpha s_F} \\\\ =\\frac{(1-\\beta)s\\frac{\\theta}{(1+\\theta)}}{(1-\\beta)s\\frac{\\theta}{(1+\\theta)}+\\alpha s\\frac{\\theta}{(1+\\theta)}}=\\frac{(1-\\beta)\\theta}{(1-\\beta)\\theta+\\alpha}$$\n:::\n\n::: {.blockquote}\n$$\\text{recall}=\\frac{TP}{(TP+FN)}=\\frac{(1-\\beta)s_T}{(1-\\beta)s_T+\\beta s_T}=1-\\beta$$\n:::\n\n## A mathematical argument (Lopez de Prado 2020)\n\n- What the mathematical logic tells us is before running backtests on a strategy, researchers should gather evidence that a strategy may indeed exist. \n- The reason is, the precision of the test is a function of the odds ratio $\\theta$. \n- If the odds ratio is low, the precision will be low, even if we get a positive with high confidence (low p-value). \n\n::: {.blockquote}\nThis is evidence to the pitfall that p-values report a rather uninformative probability. It is possible for a statistical test to have high confidence (low p-value) and low precision.\n:::\n\nIn particular, a strategy is more likely false than true if $(1-\\beta)\\theta < \\alpha$ such that precision is less than 50%.\n\n- Finally, there is an important relationship between the false discovery rate (FDR) and precision.\n- Specifically, \n\n$$FDR=\\frac{FP}{(FP+TP)}=\\frac{\\alpha}{(1-\\beta)\\theta+\\alpha}=1-precision$$\n\n##  A FDR function\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n- The following is a simple function which calculates precision, recall and the false discovery rate. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfdr_anal <- function(ground_truth, alpha = 0.05, beta, trails) {\n  theta = ground_truth / (1 - ground_truth)\n  recall = 1 - beta          \n  b1 = recall * theta\n  precision = b1 / (b1 + alpha)\n  tibble(Recall = recall, Precision = precision, FDR = 1 - precision)\n}\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n- Suppose before running backtests on a strategy, the researcher knows the *truth* that there is a 1% chance that the strategy is profitable.\n- If she sticks with the standard convention of 5% significance level and a 20% chance of a false negative, and runs 1000 trails, what is the rate of false discoveries?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfdr_anal(0.01, beta = 0.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  Recall Precision   FDR\n   <dbl>     <dbl> <dbl>\n1    0.8     0.139 0.861\n```\n\n\n:::\n:::\n\n\n- For this reason alone, we should expect that most discoveries in financial econometrics are likely false.\n:::\n\n:::\n\n## Familywise Error Rate (FWER)\n\n- When Neyman and Pearson [1933] proposed this framework, they did not consider the possibility of conducting multiple tests and select the best outcome.\n- When a test is repeated multiple times, the combined $\\alpha$ increases.\n- Consider that we repeat for a second time a test with false positive probability $\\alpha$.\n- At each trial, the probability of not making a Type I error is $1-\\alpha$\n- If the two trials are independent, the probability of not making a Type I error on the first and second tests is $(1-\\alpha)^2$\n- The probability of making *at least one* Type I error is the complementary, $1-(1-\\alpha)^2$\n- After a *family* of K independent tests, we reject H<sub>0</sub> with confidence $(1-\\alpha)^K$\n- FWER is the probability that at least one of the positives is false, $\\alpha_K=1-(1-\\alpha\n\n- FWER the probability that at least one of the positives is false, $\\alpha_K=1-(1-\\alpha)^K$\n\n- The Sidak Correction: for a given K and $\\alpha_K$ then $\\alpha=1-(1-\\alpha_K)^{1/K}$\n\n\n## FWER vs FDR {.small}\n\n- Thus far we have defined 2 Type 1 errors for multiple testing:\n1. Familywise Error Rate (FWER): The probability that at least one false positive takes place.\n2. False Discovery Rate (FDR): Expected value of the ratio of false positives to predicted positives.\n- In most scientific and industrial applications, FWER is considered overly punitive.\n  - For example, it would be impractical to design a car model where we control for the probability that a single unit will be defective.\n\n## FWER vs FDR {.small}\n  \n- However, in the context of finance, the FDR is preferrred as an investor does not typically allocate funds to all strategies with predicted positives within a family of trials, where a proportion of them are likely to be false.\n- Instead, investors are only introduced to the single best strategy out of a family of thousands or even millions of alternatives\n- Investors have no ability to invest in the discarded predicted positives.\n- Following the car analogue, in finance there is actually a single car unit produced per model, which everyone will use. If the only produced unit is defective, everyone will crash.]\n\n\n## What does this all mean for quantitative finance\n\n- Selection bias under multiple backtesting makes it impossible to assess the probability that a strategy is false.\n\n- Lopez de Prado (2018) argues that this explains why most quantitative investment firms fail as they are likely investing in false positives\n\n- This is because most financial analysts typically assess performance on the Sharpe ratio, not precision and recall.\n\n- Lopez de Prado (2020) develops a framework to assess the probability that a strategy is false, using the Sharpe ratio estimate and metadata from the discovery process as inputs\n\n\n## The golden age of the Sharpe Ratio (1966-2012)\n\n- In 1966, William Sharpe proposed a ratio metric that would come to dominate investment strategy appraisal \n- Consider an investment strategy with excess returns (or risk premia) $r_t, t=1,...,T$ which follows an IID Normal distribution\n$$ r_t \\sim N(\\mu,\\sigma)$$\n- Non-annualised SR of such a strategy is defined as \n$$SR=\\frac{\\mu}{\\sigma}$$\n- as the parameters $\\mu \\text{ and } \\sigma$ are unknown, they must be estimated such that SR is estimated as:\n\n$$\\hat{SR}=\\frac{E(r_t)}{\\sqrt{V_{r_t}}}$$\n\n## 2002 Andrew Lo and Elmar Mertens\n- [Andrew Lo](https://mitsloan.mit.edu/faculty/directory/andrew-w-lo) show that under the assumption that $r_t \\overset{IID}{\\sim} N(\\mu,\\sigma)$ the asymptotic  distribution of $\\hat{SR}$ is\n\n$$(\\hat{SR}-SR) \\overset{a}{\\to} N \\left[0,\\frac{1+0.5SR^2}{T}\\right]$$\n\n- Subsequent evidence showed hedge fund returns exhibit substantial negative skewness, and positive excess kurtosis.\n- the implication being that assumed IID normal returns will grossly underestimate the false positive probability\n\n- [Elmar Mertons](http://www.elmarmertens.com/) then derived an asymptotic distribution for $\\hat{SR}$ that include a variance terms which incorporated skewness and kurtosis.\n\n\n## 2012 David Bailey and Marco lopez de Prado\n\n- In the Journal of Risk, [David Bailey](https://www.davidhbailey.com/) and [Marco Lopez de Prado](https://www.quantresearch.org/) utilises previous results to derive the [Probabilistic Sharpe Ratio](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1821643)\n\n::: {.blockquote}\n- PSR estimates the probability that the observed $\\hat{SR}$ exceeds SR* as:\n\n$$\\hat{PSR}(SR*)=Z\\left(\\frac{(\\hat{SR}-SR*)\\sqrt{T-1}}{\\sqrt{1-\\hat{\\gamma_3}\\hat{SR}+\\frac{\\hat{\\gamma_4}-1}{4}\\hat{SR}^2}}\\right)$$\n\n- where Z[.] is the cumulative density function of the standard Normal distribution, T is number of observed returns, and $\\hat{SR}$ is the non-annualised estimate of SR, computed on the same frequency as the T observations. \n\n\n## Inference\n- For a given SR*, the probabilistic sharpe ratio increases with greater mean returns, lower variance of returns, longer track record (T), positively skewed returns, and thinner tails\n\n## The False Strategy Theorem \n- Bailey et al. (2014) formalised a theorem ,*False Strategy Theorem* , that expressed the SBuMT as a function on the number of trails and the variance of the Sharpe ratios.\n\n::: {.blockquote}\n\n- In practice a researcher may carry out a large number of historical simulations (trails) and report only the best outcome (maximum Sharpe ratio)\n- Maximum Sharpe ratio is not randomly distributed which gives rise to *SBuMT*, so when more than one trail takes place the maximum Sharpe ration is greater than the expected value of the Sharpe ration from a random trail.\n- The theorem shows that given a investment strategy with an expected Sharpe ratio of zero and non-zero variance, the expected value of the maximum Sharpe ratio is strictly positive and a function of the number of trails\n:::\n\n## The False Strategy Theorem\n\n- Given a sample of IID-Gaussian Sharpe ratios $\\widehat{SR_k},k=1,..,K \\text{  with } \\widehat{SR_k} \\sim N(0,V(\\widehat{SR_k})$\n\n\n$$E(\\underset{k}max(\\widehat{SR_k}))V(\\widehat{SR_k}^{-0.5} \\approx (1-\\gamma)Z^{-1} \\left[1-\\frac{1}{K}\\right]+\\gamma Z^{-1}\\left[1-\\frac{1}{Ke}\\right] $$\n- where $Z^{-1}$ is the inverse of the standard Gaussian CDF, e is Euler's number, and $\\gamma $is the Euler-Mascheroni constant.\n- .heatinline[Corollary:] Unless $\\underset{k}max(\\widehat{SR_k}) >> E(\\underset{k}max(\\widehat{SR_k}))$  the discovered strategy is likely to be a false positive.\n\n- But $E(\\underset{k}max(\\widehat{SR_k}))$ is usually unknown, ergo SR is dead.\n\n\n## The *False Strategy* theorem\n- .Lopez de Prado (2020) `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z\"/></svg>`{=html} code\n- The theorem can be used to express the magnitude of the SBuMT as the difference between the expected maximum Sharpe ratio and the expected Sharpe ratio of a *false* strategy from a random trail\n```\nimport numpy as np,pandas as pd\nfrom scipy.stats import norm,percentileofscore\ndef getExpectedMaxSR(nTrials,meanSR,stdSR):\n  emc=0.577215664901532860606512090082402431042159336\n  sr0=(1-emc)*norm.ppf(1-1./nTrials)+/\n  emc*norm.ppf(1-(nTrials*np.e)**-1) \n  sr0=meanSR+stdSR*sr0\n  return sr0\n```\n## Translation into `R`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngetExpectedMaxSR<-function(nTrails,meanSR,stdSR){\n  # Expected Max SR controlling for SBuMT\n  emc=0.577215664901532860606512090082402431042159336\n  sr0=(1-emc)*qnorm(p=1-1./nTrails)+emc*qnorm(1-(nTrails*exp(1))^(-1))\n  sr0=meanSR+stdSR*sr0\n  return(sr0)\n}\n```\n:::\n\n\n## Distribution of Maximum SR\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngetDistMaxSR<-function(nSims,nTrails,meanSR,stdSR){\n  out=tibble(\"Max{SR}\"=NA,\"nTrails\"=NA)\n  for (nTrails_ in nTrails) {\n    #1) Simulated Sharpe Ratios\n    set.seed(nTrails_)\n    sr<-array(rnorm(nSims*nTrails_),dim = c(nSims,nTrails_))\n    sr<-apply(sr,1,scale) # demean and scale\n    sr= meanSR+sr*stdSR\n    #2) Store output\n    out<-out %>% bind_rows(\n      tibble(\"Max{SR}\"=apply(sr,2,max),\"nTrails\"=nTrails_))\n  }\n  return(out)\n}\n```\n:::\n\n\n## Run the experiment\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(pracma)\n# Create a sequential on the log-linear scale\nnTrails<-as.integer(logspace(1,4,100)) %>% unique()\nplot(nTrails)\nsr0=array(dim = length(nTrails))\nfor (i in seq_along(nTrails)) {\n  sr0[i]<-getExpectedMaxSR(nTrails[i],meanSR = 0, stdSR = 1)\n}\nsr1=getDistMaxSR(nSims = 1000,nTrails = nTrails,meanSR = 0,stdSR = 1)\n```\n:::\n\n\n## Most important plot in Quantitative finance {.small}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n<img src=\"img/maxSR.png\">\n\n\n## Inference from plot {.small}\n- The experiment compares the empirical (Monte Carlo) estimate of Maximum Sharpe ratio under the null of a false strategy to that implied by the FS theorem\n- The plot shows the output of the experiment for 1 to 10,000 trails.\n- The code sets $V[\\hat{SR_k}]=1$ and simulates the maximum Sharpe ratio 500 times, to derive a distribution of maximum Sharpe ratios for any k (number of trails).\n- the y axis shows the distribution of the $max_k(\\hat{SR_k})$ and the Expect  \n- this results is profound, after only 100 independent backtests the expected maximum Sharpe ratio is 3.2, even when the true Sharpe ratio is zero.\n- The reason is **Backtest overfitting**: when selection bias (picking the best results) takes place under multiple testing (running many alternative configurations) that backtests are likely to be false discoveries.\n\n\n## A Solution {.small}\n\n- [The Deflated Sharpe Ratio](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2460551) computes the probability that the Sharpe Ratio (SR) is statistically significant.\n\n$$\\widehat{DSR} \\equiv \\widehat{PSR}(\\widehat{SR_0})=Z \\left[\\frac{(\\hat{SR}-E[max_k(\\widehat{SR_k})])\\sqrt{T-1}}{\\sqrt{1-\\hat{\\gamma_3}\\widehat{SR}+\\frac{\\hat{\\gamma_4}-1}{4}\\widehat{SR}^2}}\\right]$$ \n- $\\widehat{DSR}$ can be interpreted as the probability of observing a Sharpe ratio greater or equal to $\\widehat{SR}$ subject to the null hypothesis that the true Sharpe ratio is zero, while adjusting for skewness $\\gamma_3$, kurtosis $\\gamma_4$, sample length and multiple testings.\n\n- Calculate DSR requires the estimation $E[max_k(\\widehat{SR_k})])$ which requires estimating $K$ and \n$V(\\hat{SR})$ which is where FML can help.\n\n- Specifically, we are employ optimal number of clustering to estimate K the effective number of trails and then calculate the variances.  \n\n## Implications  for Academics {.small}\n::: columns\n::: column\n- Most studies in empirical finance are false Harvey et al. (2016)\n- Selection bias may invalidate the entire body of work performed for the past 100 years. \n- Finance cannot survive as a discipline unless we solve this problem.\n- Unless we learn to prevent them, investors and regulators have no reason to trust the value added by researchers and asset managers.\n\n:::\n::: column\n\n- Applying the False Strategy theorem to the prevention of false positives in finance. \n- This requires the estimation of two meta-research variables that allow us to discount for the likelihood of “lucky findings.”\n- Given that this method appears to be accurate and relatively easy to implement, academic journals should cease to accept papers that do not control for selection bias under multiple testing.\n- In particular, papers must report the probability that the claimed financial discovery is a false positive.\n:::\n:::\n\n## Implications for Regulators {.small}\n\n- Before the Food and Drug Administration (FDA) was created, adulteration and mislabeling of food and drugs caused frequent episodes of mass poisoning, birth defects and death. Such calamities only stopped through the enforcement of minimum research quality standards that prevented false positives.\n- Every year, financial firms engaging in backtest overfitting defraud investors for tens of billions of dollars. It is, perhaps, the greatest fraud in financial history. It will only worsen as more powerful computers allow for an ever-larger number of trials. The financial firms of today are the pharmaceutical firms of 100 years ago.\n- We hope that the machine learning tools presented in this paper will empower the Securities and Exchange Commission (SEC) and other regulatory agencies worldwide to take a more active role in stopping this rampant financial scam.\n– The SEC could demand that, going forward, quantitative firms that promote new investments must certify the probability that the proposed advice is simply bogus (false positive probability)\n– Quantitative firms should be required to store all trials involved in a discovery, so that a post-mortem analysis can be conducted after an investment fails to perform as advertised\n\n\n## Implications for investors {.small}\n- Many financial firms pray on the public’s trust in science.\n- They promote pseudo-scientific products arguments as scientific.\n- Investors must understand that investment products based on award-winning journal articles are not necessarily scientific.\n– The authors never reported the number of trials involved in a discovery, and therefore we must assume the discovery is false\n– Firms have all the incentive to promote those journal articles, and make a fortune by charging fees (agency problem)\n- One cynical argument is this: If the original author has not become rich with the discovery, what are my chances I will? The firm will make money regardless.\n- For every financial product or investment advice, investors must demand that firms report the results of all trials, not only the best-looking ones.\n- Investors should consult databases of investment forecasts, and assess the credibility of gurus and financial firms, based on all outcomes from past predictions investment funds (control for survivorship bias).\n\n\n---\nclass: middle\n# References\n\n.small[\n[Gu, Shihao, Bryan Kelly, and Dacheng Xiu. 2020. “Empirical Asset Pricing via Machine Learning.” The Review of Financial Studies, Working Paper Series, , February.](https://doi.org/10.1093/rfs/hhaa009)\n\nHarvey, Campbell R., Presidential Address: The Scientific Outlook in Financial Economics (July 17, 2017). Duke I&E Research Paper No. 2017-05, Available at SSRN: https://ssrn.com/abstract=2893930 or http://dx.doi.org/10.2139/ssrn.2893930\n\nAmerican Statistical Association (2016): “Ethical guidelines for statistical practice.” Committee on Professional Ethics of the American Statistical Association (April). Available at http://www.amstat.org/asa/files/pdfs/EthicalGuidelines.pdf\n\nLópez de Prado, M. and M. Lewis (2018): “Detection of False Investment Strategies Using Unsupervised Learning Methods.” Working Paper. Available at https://ssrn.com/abstract=3167017⁄\n\nBailey, D., J. Borwein, M. López de Prado, and J. Zhu (2014a): “Pseudo- mathematics and financial charlatanism: The effects of backtest overfitting on out-of-sample performance.” Notices of the American Mathematical Society, Vol. 61, No. 5, pp. 458–471. Available at http://ssrn.com/abstract=2308659\n\nBailey, D., J. Borwein, M. López de Prado, and J. Zhu (2017): “The Probability of Backtest Overfitting.” Journal of Computational Finance, Vol. 20, No. 4, pp. 39-70. Available at http://ssrn.com/abstract=2326253\n\nBailey, D. and M. López de Prado (2012): “The Sharpe ratio efficient frontier.” Journal of Risk, Vol. 15, No. 2, pp. 3–44.\n\nBailey, D. and M. López de Prado (2014): “The deflated Sharpe ratio: Correcting for selection bias, backtest overfitting and non-normality.” Journal of Portfolio Management, Vol. 40, No. 5, pp. 94-107.\n\nChristie, S. (2005): “Is the Sharpe Ratio Useful in Asset Allocation?” MAFC Research Paper No. 31, Applied Finance Centre, Macquarie University.Electronic copy available at: https://ssrn.com/abstract=3261943\n\nHarvey, C., Y. Liu and C. Zhu (2016): “...and the Cross-Section of Expected Returns.” Review of Financial Studies, 29(1), pp. 5-68. Available at https://ssrn.com/abstract=2249314\n\nLo, A. (2002): “The Statistics of Sharpe Ratios.” Financial Analysts Journal (July), pp. 36-52.\nLópez de Prado, M. (2016a): “Building Diversified Portfolios that Outperform Out- of-Sample.” Journal of Portfolio Management, Vol. 42, No. 4, pp. 59-69.\n]\n\n\n## Further Reading\n\nLópez de Prado, M. (2016b): “Mathematics and Economics: A reality check.” Journal of Portfolio Management, Vol. 43, No. 1, pp. 5-8.\n\nLópez de Prado, M. (2017): “Finance as an Industrial Science.” Journal of Portfolio Management, Vol. 43, No. 4, pp. 5-9.\n\nLópez de Prado, M. (2018): Advances in Financial Machine Learning. 1st edition, Wiley. https://www.amazon.com/dp/1119482089\n\nMertens, E. (2002): “Variance of the IID estimator in Lo (2002).” Working paper, University of Basel.\n\nOpdyke, J. (2007): “Comparing Sharpe ratios: So where are the p-values?” Journal of Asset Management, Vol. 8, No. 5, pp. 308–336.Electronic copy available at: https://ssrn.com/abstract=3261943\n\n• Rousseeuw, P. (1987): “Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Computational and Applied Mathematics, Vol. 20, pp. 53–65.\n\n• Sharpe, W. (1966): “Mutual Fund Performance”, Journal of Business, Vol. 39, No. 1, pp. 119–138.\n\n• Sharpe, W. (1975): “Adjusting for Risk in Portfolio Performance Measurement\", Journal of Portfolio Management, Vol. 1, No. 2, Winter, pp. 29-34.\n\n• Sharpe, W. (1994): “The Sharpe ratio”, Journal of Portfolio Management, Vol. 21, No. 1, Fall, pp. 49-58.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}