---
title: "Backtesting Overfitting: In Practice"
subtitle: "Practical Implementation and Recent Advances"
author: "Barry Quinn"
footer: "AI and Trading"
embed-resources: true
logo: "img/qbslogo.png"
format:
  revealjs:
    includes:
    css: ["mycssblend.css"]
    theme: default
    transition: slide
    slide-number: true
    scrollable: true
    width: 1600
    height: 900
execute:
  echo: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width = 8, fig.height = 4, fig.align = 'center', dpi = 300)
library(DataExplorer)
library(xaringanExtra)
library(kableExtra)
library(fontawesome)
library(tidyverse)
library(plotly)
library(factoextra)
library(cluster)
library(PortfolioAnalytics)
```

## From Theory to Practice

::: {.saltinline}
- We've covered the theoretical foundations of backtest overfitting
- Now we'll focus on practical implementation
- Key questions:
  - How do we calculate the Deflated Sharpe Ratio in practice?
  - How do we estimate the effective number of trials?
  - What practical workflows can prevent false discoveries?
:::

## A Practical DSR Workflow

:::: {.columns}

::: {.column width="50%"}
1. Strategy development and backtesting
2. Estimation of effective number of trials ($K$)
3. Estimation of Sharpe ratio variance
4. Calculation of expected maximum Sharpe ratio
5. Calculation of Deflated Sharpe Ratio
6. Evaluation against DSR threshold
:::

::: {.column width="50%"}
![](img/dsr_workflow.png){.center}
:::

::::

## Estimating the Effective Number of Trials

- In practice, strategies are often highly correlated
- The effective number of independent trials ($K$) is typically much lower than the total number of configurations tested
- Methods to estimate $K$:
  1. Clustering of strategy returns
  2. Principal Component Analysis
  3. Researcher's logs of configuration tests

## Clustering Approach to Effective Trials

```{r clustering_example}
# Generate correlated strategy returns
set.seed(42)
n_strategies <- 50
n_returns <- 252
base_returns <- matrix(rnorm(10 * n_returns), nrow = n_returns)

# Create strategies with varying correlations to base returns
strategies_returns <- matrix(0, nrow = n_returns, ncol = n_strategies)
for(i in 1:n_strategies) {
  # Mix of base returns and unique noise
  weight <- runif(1, 0.3, 0.9)
  base_idx <- sample(1:10, 1)
  strategies_returns[,i] <- weight * base_returns[,base_idx] + 
                           (1-weight) * rnorm(n_returns)
}

# Calculate correlation matrix
cor_matrix <- cor(strategies_returns)
# Convert to distance matrix
dist_matrix <- as.dist(1 - abs(cor_matrix))
# Hierarchical clustering
hc <- hclust(dist_matrix, method = "complete")

# Plot dendrogram
plot(hc, main = "Hierarchical Clustering of Strategy Returns", 
     xlab = "", sub = "", cex = 0.6)
rect.hclust(hc, k = 8, border = "red")
```

## Visualizing Effective Number of Trials

```{r silhouette_method}
# Determine optimal number of clusters using silhouette method
fviz_nbclust(strategies_returns, FUN = hcut, method = "silhouette", 
             k.max = 15, diss = dist_matrix) +
  labs(title = "Optimal Number of Clusters",
       subtitle = "Using Silhouette Method")

# Cut tree at optimal number
k_opt <- 8  # Based on silhouette plot
clusters <- cutree(hc, k = k_opt)

# Show the first few strategies and their cluster assignments
head(tibble(Strategy = 1:n_strategies, Cluster = clusters), 10)
```

## The Link Between Clustering and Effective Trials

::: {.blockquote}
The effective number of independent trials ($K_{eff}$) is approximately equal to the optimal number of clusters when strategies are grouped by similarity in returns.
:::

```{r effective_trials}
# Calculate the number of strategies in each cluster
cluster_sizes <- table(clusters)

# Estimate effective number of trials
k_eff <- length(cluster_sizes)

# Display results
cat("Total strategies tested:", n_strategies, "\n")
cat("Effective number of independent trials:", k_eff, "\n")
cat("Reduction factor:", round(n_strategies/k_eff, 2), "\n")
```

## Calculating DSR with Estimated Effective Trials

```{r dsr_calculation}
# Function to calculate DSR
calculate_dsr <- function(strategy_returns, n_effective_trials, 
                          mean_sr = 0, sr_variance = NULL) {
  # Calculate Sharpe ratio and its components
  n <- length(strategy_returns)
  sr <- mean(strategy_returns) / sd(strategy_returns)
  
  # Calculate skewness and kurtosis
  z <- (strategy_returns - mean(strategy_returns)) / sd(strategy_returns)
  skew <- sum(z^3) / n
  kurt <- sum(z^4) / n - 3  # Excess kurtosis
  
  # If SR variance not provided, estimate it
  if (is.null(sr_variance)) {
    sr_variance <- 1  # Simplification for example
  }
  
  # Calculate expected maximum SR
  emc <- 0.577215664901532860606512090082402431042159336  # Euler-Mascheroni
  exp_max_sr <- (1 - emc) * qnorm(p = 1 - 1/n_effective_trials) + 
               emc * qnorm(1 - (n_effective_trials * exp(1))^(-1))
  exp_max_sr <- mean_sr + sqrt(sr_variance) * exp_max_sr
  
  # DSR calculation
  numerator <- (sr - exp_max_sr) * sqrt(n - 1)
  denominator <- sqrt(1 - skew * sr + (kurt / 4) * sr^2)
  dsr <- pnorm(numerator / denominator)
  
  return(list(
    sharpe_ratio = sr,
    expected_max_sr = exp_max_sr,
    dsr = dsr
  ))
}

# Calculate DSR for a sample strategy
sample_strategy <- strategies_returns[,1]
dsr_results <- calculate_dsr(
  sample_strategy, 
  n_effective_trials = k_eff
)

# Display results
cat("Strategy Sharpe Ratio:", round(dsr_results$sharpe_ratio, 4), "\n")
cat("Expected Max SR with", k_eff, "trials:", 
    round(dsr_results$expected_max_sr, 4), "\n")
cat("Deflated Sharpe Ratio:", round(dsr_results$dsr, 4), "\n")
```

## Visual Representation of Precision and FDR

```{r precision_fdr_visual}
# Function to calculate precision and FDR
calculate_precision_fdr <- function(theta, alpha = 0.05, beta = 0.2) {
  recall <- 1 - beta          
  b1 <- recall * theta
  precision <- b1 / (b1 + alpha)
  fdr <- 1 - precision
  return(c(precision = precision, fdr = fdr))
}

# Calculate precision and FDR for different theta values
theta_values <- seq(0.001, 0.5, by = 0.001)
results <- t(sapply(theta_values, calculate_precision_fdr))
results_df <- tibble(
  theta = theta_values,
  precision = results[, "precision"],
  fdr = results[, "fdr"]
)

# Plot
ggplot(results_df, aes(x = theta)) +
  geom_line(aes(y = precision, color = "Precision"), size = 1) +
  geom_line(aes(y = fdr, color = "FDR"), size = 1) +
  scale_color_manual(values = c("Precision" = "blue", "FDR" = "red")) +
  labs(
    title = "Precision and False Discovery Rate vs. Odds Ratio",
    subtitle = "Alpha = 0.05, Beta = 0.2",
    x = "Theta (Odds Ratio of True vs. False Strategies)",
    y = "Rate",
    color = "Metric"
  ) +
  theme_minimal() +
  geom_vline(xintercept = 0.05/0.8, linetype = "dashed") +
  annotate("text", x = 0.07, y = 0.5, 
           label = "Precision = 50%\nwhen θ = α/(1-β)")
```

## Recent Advances: Combinatorial Purged Cross-Validation

- Introduced by Lopez de Prado (2018)
- Addresses two key problems in financial machine learning:
  1. Leakage from training to test sets due to serial correlation
  2. Selection bias under multiple testing

::: {.blockquote}
CPCV provides a framework for model selection that:
- Purges training observations that overlap with test observations
- Embargoes observations that follow test observations
- Generates multiple train/test splits to assess model variance
:::

![](img/cpcv.png){.center width="60%"}

## Walk-Forward Testing vs. CPCV

:::: {.columns}

::: {.column width="50%"}
**Walk-Forward Testing**
- Traditional approach in finance
- Training window followed by test window
- Windows move forward in time
- Limited number of test samples
- Does not fully address selection bias
:::

::: {.column width="50%"}
**Combinatorial Purged CV**
- Training and test sets across all available data
- Purging of overlapping observations
- Embargo of subsequent observations
- Many more test samples
- Better estimate of out-of-sample performance
:::

::::

![](img/walkforward_vs_cpcv.png){.center width="70%"}

## Practical Implementation of CPCV

```{r cpcv_demo, eval=FALSE}
# This is pseudocode for demonstration purposes
implement_cpcv <- function(returns, feature_data, model_func, 
                           n_splits = 5, purge_window = 20, embargo = 5) {
  
  # Define time indices
  T <- length(returns)
  indices <- 1:T
  
  # Create time-based folds
  fold_size <- floor(T / n_splits)
  folds <- list()
  
  for(i in 1:n_splits) {
    test_start <- (i-1) * fold_size + 1
    test_end <- min(i * fold_size, T)
    test_indices <- test_start:test_end
    
    # Apply purging: remove from training observations that overlap with test
    purge_before <- max(1, test_start - purge_window)
    purge_after <- min(T, test_end + purge_window)
    purge_indices <- purge_before:purge_after
    
    # Apply embargo: remove from training observations that follow test
    embargo_end <- min(T, test_end + embargo)
    embargo_indices <- (test_end + 1):embargo_end
    
    # Training indices are all indices except test, purge, and embargo
    train_indices <- setdiff(indices, unique(c(test_indices, purge_indices, embargo_indices)))
    
    folds[[i]] <- list(train = train_indices, test = test_indices)
  }
  
  # Run model on each fold
  results <- list()
  for(i in 1:length(folds)) {
    train_data <- feature_data[folds[[i]]$train, ]
    train_returns <- returns[folds[[i]]$train]
    test_data <- feature_data[folds[[i]]$test, ]
    test_returns <- returns[folds[[i]]$test]
    
    # Train model
    model <- model_func(train_data, train_returns)
    
    # Predict on test data
    predictions <- predict(model, test_data)
    
    # Evaluate performance
    performance <- evaluate_performance(predictions, test_returns)
    
    results[[i]] <- performance
  }
  
  return(results)
}
```

## Making Investment Decisions with DSR

:::: {.columns}

::: {.column width="60%"}
**DSR Thresholds for Strategy Selection**
- DSR < 0.5: Likely false discovery (reject)
- 0.5 ≤ DSR < 0.95: Possible true discovery (further testing)
- DSR ≥ 0.95: Likely true discovery (accept)

**Implementation Considerations**
- Monitor DSR through time
- Re-evaluate strategies when DSR drops
- Allocate capital based on DSR confidence
- Diversify across uncorrelated strategies
:::

::: {.column width="40%"}
```{r dsr_threshold, echo=FALSE}
# Create data for demonstration
dsr_values <- c(0.25, 0.55, 0.65, 0.98)
strategy_names <- c("Strategy A", "Strategy B", "Strategy C", "Strategy D")
sharpe_values <- c(1.8, 2.2, 2.0, 1.95)
decision <- c("Reject", "Further Testing", "Further Testing", "Accept")

dsr_df <- tibble(
  Strategy = strategy_names,
  Sharpe = sharpe_values,
  DSR = dsr_values,
  Decision = decision
)

# Create table
kable(dsr_df, format = "html", caption = "Strategy Selection Framework") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(3, color = "white", 
              background = ifelse(dsr_values < 0.5, "red", 
                         ifelse(dsr_values >= 0.95, "green", "orange")))
```
:::

::::

## Meta-Labeling: A Complementary Approach

::: {.blockquote}
Meta-labeling separates the problem of side prediction (buy/sell) from the problem of bet sizing.
:::

- **First Model**: Predicts the direction (e.g., using technical indicators)
- **Second Model**: Predicts the probability of success for the first model's predictions

:::: {.columns}

::: {.column width="50%"}
**Benefits**
- Addresses class imbalance problem
- Reduces false positives
- Provides natural bet sizing
- Complements DSR framework
:::

::: {.column width="50%"}
**Implementation**
1. Develop primary model for direction
2. Label outcomes (success/failure)
3. Train secondary model to predict success
4. Size positions based on success probability
:::

::::

## Bayesian Approaches to Backtest Evaluation

- Traditional backtesting (frequentist) is vulnerable to overfitting
- Bayesian methods offer advantages:
  - Incorporation of prior beliefs
  - Full posterior distributions instead of point estimates
  - Natural handling of model uncertainty

:::: {.columns}

::: {.column width="50%"}
**Bayesian Sharpe Ratio**
- Assumes a prior distribution for the Sharpe ratio
- Updates based on observed returns
- Results in posterior distribution
- Provides probability intervals for true SR
:::

::: {.column width="50%"}
```{r bayesian_sr, echo=FALSE}
# Create example of Bayesian SR
x <- seq(-1, 3, length.out = 1000)

# Prior (diffuse)
prior <- dnorm(x, mean = 0, sd = 1)

# Likelihood (based on observed SR = 1.5 with some uncertainty)
likelihood <- dnorm(x, mean = 1.5, sd = 0.5)

# Posterior (simplified calculation for illustration)
posterior <- dnorm(x, mean = 1.2, sd = 0.3)

# Plot
plot_data <- tibble(
  x = rep(x, 3),
  density = c(prior, likelihood, posterior),
  type = rep(c("Prior", "Likelihood", "Posterior"), each = length(x))
)

ggplot(plot_data, aes(x = x, y = density, color = type)) +
  geom_line(size = 1) +
  labs(
    title = "Bayesian Sharpe Ratio Estimation",
    x = "Sharpe Ratio",
    y = "Density",
    color = "Distribution"
  ) +
  theme_minimal() +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5)
```
:::

::::

## Complexity-Adjusted Performance Metrics

- Strategy complexity is a key factor in overfitting
- More complex strategies have more degrees of freedom
- Complexity-adjusted metrics penalize complexity:

:::: {.columns}

::: {.column width="60%"}
**Information-Theoretic Approaches**
- Akaike Information Criterion (AIC)
- Bayesian Information Criterion (BIC)
- Minimum Description Length (MDL)

**Regularization Techniques**
- Ridge regression (L2 penalty)
- LASSO regression (L1 penalty)
- Elastic Net (combination of L1 and L2)
:::

::: {.column width="40%"}
**Example: AIC for Strategy Selection**
```{r complexity_metrics, echo=FALSE}
# Create example of model complexity comparison
strategies <- c("Moving Average Crossover", 
                "Bollinger Band Strategy",
                "Multi-factor Model",
                "Deep Neural Network")
parameters <- c(2, 5, 12, 150)
sharpe <- c(1.2, 1.5, 1.8, 2.1)
aic <- c(120, 150, 210, 350)
complexity_adj_sharpe <- sharpe - 0.1 * log(parameters)

complexity_df <- tibble(
  Strategy = strategies,
  Parameters = parameters,
  `Sharpe Ratio` = sharpe,
  AIC = aic,
  `Adjusted Sharpe` = complexity_adj_sharpe
)

kable(complexity_df, format = "html", 
      caption = "Complexity-Adjusted Strategy Performance") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(which.max(complexity_adj_sharpe), background = "#e6ffe6")
```
:::

::::

## Ethical Considerations in Strategy Development

::: {.saltinline}
- Researchers have ethical responsibility to report honest results
- Investors trust performance metrics for capital allocation
- Regulators rely on accurate disclosures
:::

**Best Practices:**
1. Pre-register testing protocols
2. Report all trials and configurations
3. Disclose DSR alongside Sharpe ratio
4. Maintain research logs for audit trail
5. Use out-of-sample validation periods

## Industry Implementation: Case Study

::: {.blockquote}
**AQR Capital Management**: One of the pioneers in addressing backtest overfitting

Cliff Asness (AQR co-founder): "We aim to publish strategies with high out-of-sample Sharpe ratios, not just high backtest Sharpe ratios."
:::

**AQR's Approach:**
- Long out-of-sample periods (often decades)
- Focus on economically justified factors
- Implementation across multiple asset classes
- Transparency in methodology
- Publication of research and results

## Putting It All Together: A Robust Workflow

![](img/robust_workflow.png){.center width="80%"}

## Summary of Key Extensions

- Practical implementation of DSR calculation
- Methods to estimate effective number of trials
- Combinatorial Purged Cross-Validation
- Meta-labeling for bet sizing
- Bayesian approaches to backtest evaluation
- Complexity-adjusted performance metrics
- Ethical considerations and best practices

## Workshop Exercises

1. **False Discovery Estimation**
   - Simulate strategy development and selection
   - Calculate false discovery rates
   - Implement DSR to correct for selection bias

2. **Robust Strategy Evaluation Framework**
   - Develop walk-forward testing procedure
   - Estimate effective number of trials
   - Create decision framework for strategy selection

## References

- Lopez de Prado, M. (2018). "Advances in financial machine learning." John Wiley & Sons.
- Lopez de Prado, M. (2019). "A data science solution to the multiple-testing crisis in financial research." Journal of Financial Data Science.
- Bailey, D. H., & Lopez de Prado, M. (2014). "The deflated Sharpe ratio: Correcting for selection bias, backtest overfitting, and non-normality." Journal of Portfolio Management.
- Harvey, C. R., & Liu, Y. (2015). "Backtesting." Journal of Portfolio Management.
- Cherry, S., & Shallue, C. J. (2019). "Statistical significance and p-values in machine learning research." ArXiv preprint.
- Bollen, N. P. B., & Pool, V. K. (2009). "Do Hedge Fund Managers Misreport Returns? Evidence from the Pooled Distribution." Journal of Finance.
- Novy-Marx, R. (2016). "Testing strategies based on multiple signals." Working paper, University of Rochester.